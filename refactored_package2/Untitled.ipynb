{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Discover Thomson Reuters']\n",
      "['More']\n",
      "['By Reuters Staff']\n",
      "['2 Min Read']\n",
      "['(Reuters) - An earthquake of magnitude 6.8 struck west-central Argentina on Monday, followed by at least five aftershocks that shook buildings and sent products tumbling off supermarket shelves, but there were no immediate reports of injuries.']\n",
      "['The quake hit at a depth of 10 km (6 miles), the German Research Center for Geosciences (GFZ) said, with its epicentre located 40 km (25 miles) south of the province of San Juan.']\n",
      "['The aftershocks ranged in magnitude from 3.5 to 5, the European Mediterranean Seismological Centre (EMSC) said.']\n",
      "['Posts on social media showed many homes and buildings shaking, with the seismic activity leaving cracks in roads, while glass bottles fell off the shelves in shops.']\n",
      "['San Juan Governor Sergio UÃ±ac urged people to remain calm following the earthquake and called on them to reach out in case they need any assistance.']\n",
      "['\"Let us put into practice all the measures we have learned to prevent incidents, while we are committed to knowing the impact of the (earthquake) to collaborate in everything necessary,\" he said bit.ly/3qyOf4a in a tweet, advising those who suffered damage or in need of assistance to call the emergency number 911.']\n",
      "['No tsunami warning was issued, the U.S. Tsunami Warning System said.']\n",
      "['Reporting by Bhargav Acharya in Bengaluru; Editing by Himani Sarkar and Clarence Fernandez & Shri Navaratnam']\n",
      "['Our Standards: The Thomson Reuters Trust Principles.']\n",
      "['All quotes delayed a minimum of 15 minutes.', 'See here for a complete list of exchanges and delays.']\n",
      "['House Ethics Committee launches investigation into Congressman Gaetz, who faces a federal probe of possible sexual misconduct']\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2 \n",
    "import textract\n",
    "import re\n",
    "from nltk import sent_tokenize\n",
    "import pandas as pd\n",
    "from urllib.request import urlopen\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from PIL import Image\n",
    "import pandas as pd \n",
    "\n",
    "\n",
    "# Download ntlk\n",
    "# nltk.download()\n",
    "\n",
    "def get_content(website, dest_file=\"\"):\n",
    "    \"\"\"\n",
    "    README: This function is used to copy text (html type <p>) into an output file. \n",
    "        If failed, no text will be copied.\n",
    "\n",
    "    website: the website to grab information.\n",
    "    dest_file: output file name (and directory). *.txt recommended. Note that new text \n",
    "        will be appended behind the old text. If not specified, the text will be printed\n",
    "        (not recommend).\n",
    "\n",
    "    In order to split the sentences, package nltk is used herein.\n",
    "        nltk needs to be downloaded first by using \"nltk.download()\".\n",
    "    \"\"\"\n",
    "\n",
    "    driver = webdriver.Chrome(\"/Users/vedantmathur/Downloads/chromedriver\")\n",
    "    try:\n",
    "        driver.get(website)\n",
    "        content_element = driver.find_element_by_tag_name(\"div\")\n",
    "        content_html = content_element.get_attribute(\"innerHTML\")\n",
    "        soup = BeautifulSoup(content_html, \"html.parser\")\n",
    "        p_tags = soup.find_all(\"p\")\n",
    "        #print(content_html) \n",
    "        #print(dest_file) \n",
    "        if (dest_file != \"\"):\n",
    "          #  print(\"what\") \n",
    "          #  print(p_tags) \n",
    "            text = open(dest_file, \"w\")\n",
    "            totalSentences = [] \n",
    "            for p in p_tags:\n",
    "                #text.write(p.getText())\n",
    "                sentences = sent_tokenize(p.getText())\n",
    "                print(sentences)\n",
    "                for i in range(len(sentences)):\n",
    "                    text.write(sentences[i])\n",
    "                    text.write(\"\\n\")\n",
    "                    \n",
    "                    totalSentences.append(sentences[i]) \n",
    "            text.close()\n",
    "            \n",
    "            dataFrame = pd.DataFrame(data = {\"sentence\": totalSentences}) \n",
    "            dataFrame.to_csv(\"data.csv\") \n",
    "        else:\n",
    "            for p in p_tags:\n",
    "                sentences = sent_tokenize(p.getText())\n",
    "                for i in range(len(sentences)):\n",
    "                    print(sentences[i])\n",
    "        driver.close()\n",
    "    except:\n",
    "        driver.close()\n",
    "\n",
    "def read_pdf(filename):\n",
    "    \"\"\"\n",
    "    README: This function is used to read texts from a PDF file into a list of strings.\n",
    "        Each string in the list is one row of text.\n",
    "        This is a helper function.\n",
    "\n",
    "    filename: the file name of *.pdf file (and directory).\n",
    "    RETURN: a list of strings\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    pdfFileObj = open(filename,'rb')  \n",
    "    pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
    "    pdfReader.numPages\n",
    "    pageObj = pdfReader.getPage(3)\n",
    "    text = pageObj.extractText()\n",
    "    if (text == \"\"):\n",
    "    \"\"\"\n",
    "    text = textract.process(filename, method='tesseract', language='eng')\n",
    "    return str(text).split('\\\\n')\n",
    "\n",
    "def steer(source_file, dest_file):\n",
    "    \"\"\"\n",
    "    README: This function is used to read texts from websites referenced in STEER earthquake briefings.\n",
    "        This function calls read_pdf to locate and extract the websites inside the PDF, \n",
    "        then it iterates over websites by calling get_content to obtain the texts.\n",
    "\n",
    "    source_file: PDF file name (and directory).\n",
    "    dest_file: output file name (and directory).\n",
    "    \"\"\"\n",
    "    lines = read_pdf(source_file)\n",
    "    i = 0\n",
    "    while (i < len(lines)):\n",
    "        index = lines[i].find(\"eference\") #reference\n",
    "        if (index != -1):\n",
    "            break\n",
    "        i += 1\n",
    "    while (i < len(lines)):\n",
    "        #print(i)\n",
    "        index = lines[i].find(\"http\")\n",
    "        if (index == -1):\n",
    "            i += 1\n",
    "            continue\n",
    "        url = lines[i][index:]\n",
    "        url_found = False\n",
    "        try:\n",
    "            req = requests.get(url)\n",
    "        except:\n",
    "            i += 1\n",
    "            continue\n",
    "        if (str(req) == \"<Response [200]>\"):\n",
    "            url_found = True\n",
    "        else:\n",
    "            url += lines[i+1]\n",
    "            try:\n",
    "                req = requests.get(url)\n",
    "            except:\n",
    "                i += 1\n",
    "                continue\n",
    "            if (str(req) == \"<Response [200]>\"):\n",
    "                url_found = True\n",
    "                \n",
    "        if (not url_found):\n",
    "            url = lines[i][index:]\n",
    "            url.replace(\" \", \"\")\n",
    "            try:\n",
    "                req = requests.get(url)\n",
    "            except:\n",
    "                i += 1\n",
    "                continue\n",
    "            if (str(req) == \"<Response [200]>\"):\n",
    "                url_found = True\n",
    "            else:\n",
    "                url += lines[i+1]\n",
    "                url.replace(\" \", \"\")\n",
    "                try:\n",
    "                    req = requests.get(url)\n",
    "                except:\n",
    "                    i += 1\n",
    "                    continue\n",
    "                if (str(req) == \"<Response [200]>\"):\n",
    "                    url_found = True\n",
    "        if (not url_found):\n",
    "            url = lines[i][index:]\n",
    "            url.replace(\" \", \".\")\n",
    "            try:\n",
    "                req = requests.get(url)\n",
    "            except:\n",
    "                i += 1\n",
    "                continue\n",
    "            if (str(req) == \"<Response [200]>\"):\n",
    "                url_found = True\n",
    "            else:\n",
    "                url += lines[i+1]\n",
    "                url.replace(\" \", \".\")\n",
    "                try:\n",
    "                    req = requests.get(url)\n",
    "                except:\n",
    "                    i += 1\n",
    "                    continue\n",
    "                if (str(req) == \"<Response [200]>\"):\n",
    "                    url_found = True\n",
    "                    \n",
    "        if (not url_found):\n",
    "            i += 1\n",
    "            continue\n",
    "        #print(url)\n",
    "        get_content(url, dest_file)\n",
    "        i += 1\n",
    "        \n",
    "get_content(\"https://www.reuters.com/article/us-argentina-quake/magnitude-68-quake-strikes-san-juan-province-argentina-gfz-idUSKBN29O077\", \"article.txt\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Power' 'Water supply' 'Gas' 'Transportation' 'Retail services'\n",
      " 'Internet provider services' 'Telecommunication' 'School' 'Hospital'\n",
      " 'Office']\n",
      "['power plant', 'wind turbine', 'solar panel', 'electricity', 'power outage']\n",
      "\n",
      "defaultdict(<class 'int'>, {26.0: 1, 24.0: 2, 23.0: 2, 22.0: 2, 20.0: 2, 17.0: 1, 14.0: 2, 13.0: 2, 11.0: 3, 10.0: 10, 9.0: 16, 8.0: 45, 7.0: 43, 6.0: 47, 5.0: 22, 4.0: 1})\n",
      "['water', 'dam', 'faucet']\n",
      "\n",
      "defaultdict(<class 'int'>, {28.0: 1, 27.0: 2, 26.0: 2, 25.0: 3, 24.0: 1, 22.0: 1, 20.0: 1, 18.0: 3, 17.0: 4, 16.0: 8, 15.0: 3, 14.0: 7, 13.0: 13, 12.0: 10, 11.0: 19, 10.0: 26, 9.0: 56, 8.0: 114, 7.0: 76, 6.0: 36, 5.0: 12, 4.0: 3})\n",
      "['pipeline', 'gas', 'heat', 'refinery', 'cold']\n",
      "\n",
      "defaultdict(<class 'int'>, {30.0: 1, 29.0: 1, 28.0: 2, 27.0: 1, 26.0: 1, 24.0: 1, 23.0: 4, 22.0: 5, 20.0: 3, 19.0: 1, 16.0: 4, 15.0: 2, 14.0: 7, 13.0: 3, 12.0: 3, 11.0: 7, 10.0: 11, 9.0: 25, 8.0: 71, 7.0: 81, 6.0: 100, 5.0: 55, 4.0: 16, 3.0: 5})\n",
      "['bridge', 'highway', 'road ', 'airport', 'tunnel', 'port', 'railway', 'train', 'transportation']\n",
      "\n",
      "defaultdict(<class 'int'>, {30.0: 1, 27.0: 1, 24.0: 1, 22.0: 1, 19.0: 1, 17.0: 2, 16.0: 1, 14.0: 2, 13.0: 5, 12.0: 5, 11.0: 4, 10.0: 10, 9.0: 21, 8.0: 35, 7.0: 19, 6.0: 15, 5.0: 11, 4.0: 4})\n",
      "['restaurant', 'supermarket', 'department store', 'bank', 'food', 'grocery']\n",
      "\n",
      "defaultdict(<class 'int'>, {29.0: 1, 21.0: 1, 19.0: 1, 17.0: 2, 14.0: 2, 13.0: 5, 12.0: 3, 11.0: 6, 10.0: 12, 9.0: 17, 8.0: 21, 7.0: 17, 6.0: 14, 5.0: 7, 4.0: 1})\n",
      "['internet', 'web', 'online']\n",
      "\n",
      "defaultdict(<class 'int'>, {13.0: 1, 12.0: 1, 10.0: 1, 9.0: 1, 8.0: 5, 7.0: 6, 6.0: 7, 5.0: 5})\n",
      "['cell tower', 'transmission tower', 'telecommunication', 'phone', 'calls', 'text']\n",
      "\n",
      "defaultdict(<class 'int'>, {14.0: 1, 11.0: 3, 10.0: 3, 9.0: 3, 8.0: 9, 7.0: 7, 6.0: 17, 5.0: 5})\n",
      "['school', 'students', 'teachers', 'college', 'campus']\n",
      "\n",
      "defaultdict(<class 'int'>, {22.0: 1, 17.0: 1, 13.0: 3, 12.0: 2, 11.0: 1, 10.0: 1, 9.0: 4, 8.0: 7, 7.0: 3, 6.0: 4, 5.0: 4})\n",
      "['hospital', 'patients']\n",
      "\n",
      "defaultdict(<class 'int'>, {10.0: 1, 9.0: 1, 8.0: 3, 7.0: 1, 5.0: 1})\n",
      "['office', 'work from home', 'WFH', 'remote', 'zoom']\n",
      "\n",
      "defaultdict(<class 'int'>, {11.0: 1, 10.0: 1, 9.0: 2, 8.0: 3, 7.0: 4, 6.0: 7, 5.0: 5, 4.0: 2})\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from collections import defaultdict \n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import math\n",
    "\n",
    "data = pd.read_csv(\"dataalbania.csv\") \n",
    "chosen = ((data[\"date\"][0]))\n",
    "\n",
    "def convertToTime(inputString): \n",
    "    year = int(inputString[0:4])\n",
    "    month = int(inputString[5:7])\n",
    "    day = int(inputString[8:10])\n",
    "    hour = int(inputString[11:12 + 1])\n",
    "    minute = int(inputString[14:16])\n",
    "    seconds = int(inputString[17:19])\n",
    "\n",
    "    dateObject = datetime(year, month, day, hour, minute, seconds) \n",
    "    \n",
    "    return dateObject\n",
    "\n",
    "def dateDifference(date1, date2): \n",
    "    return divmod((convertToTime(date1) - convertToTime(date2)).total_seconds(), 3600 * 24)\n",
    "\n",
    "earthquakeLog = pd.read_csv(\"data/earthquakes_log.csv\") \n",
    "twitterCSV = pd.read_csv(\"data/twitter/tweets.csv\") \n",
    "\n",
    "def processCSV(logFile, twitterFile, country, keywords): \n",
    "    frequencies = defaultdict(int) \n",
    "    log = pd.read_csv(logFile) \n",
    "    tweets = pd.read_csv(twitterFile)\n",
    "    print()\n",
    "    tweets = tweets.to_numpy() \n",
    "    #print(log)\n",
    "    logTime = log[log[\"country\"] == country][\"rupture_time\"][0]\n",
    "    #print(logTime)\n",
    "    \n",
    "#print(twitterCSV.to_numpy())twitterCSV = twitterCSV.to_numpy()\n",
    "   # print(tweets)\n",
    "    for x in range(0, len(tweets)): \n",
    "       # print(twitterCSV[x][4])\n",
    "      \n",
    "       # if(any([keywords[j] in tweets[x][4] for j in range(0, len(keywords))])):\n",
    "        \n",
    "       # print(\"what\\n\\n\\n\\n\\n\")\n",
    "        differenceTweet = (dateDifference(tweets[x][0], logTime)[0]) \n",
    "        frequencies[differenceTweet] += 1\n",
    "            \n",
    "    print(frequencies)\n",
    "    x = frequencies.keys() \n",
    "    y = frequencies.values()\n",
    "    \n",
    "    plt.plot(x, y)\n",
    "    plt.show()\n",
    "   \n",
    "def processTexas(logFile, twitterFile, keywordCategory, keywords): \n",
    "    frequencies = defaultdict(int) \n",
    "    log = pd.read_csv(logFile) \n",
    "    tweets = pd.read_csv(twitterFile)\n",
    "    print()\n",
    "    tweets = tweets.to_numpy() \n",
    "    #print(log)\n",
    "    logTime = \"2021-02-10 00:00:00\"\n",
    "    #print(logTime)\n",
    "    \n",
    "#print(twitterCSV.to_numpy())twitterCSV = twitterCSV.to_numpy()\n",
    "   # print(tweets)\n",
    "    for x in range(0, len(tweets)): \n",
    "      #  print(tweets[x][0])\n",
    "      #  print((tweets[x][3]))\n",
    "    \n",
    "        if(any([keywords[j] in tweets[x][2] for j in range(0, len(keywords))])):\n",
    "       # print(\"what\\n\\n\\n\\n\\n\")\n",
    "            differenceTweet = (dateDifference(tweets[x][0], logTime)[0]) \n",
    "            frequencies[differenceTweet] += 1\n",
    "            \n",
    "    print(frequencies)\n",
    "    x = frequencies.keys() \n",
    "    y = frequencies.values()\n",
    "    \n",
    "    plt.plot(x, y)\n",
    "  #  plt.show()\n",
    "    plt.xlabel(\"days after 2/10\") \n",
    "    plt.ylabel(\"number of tweet\") \n",
    "    plt.title(keywordCategory)\n",
    "    \n",
    "    plt.savefig(keywordCategory + \".png\")\n",
    "    \n",
    "    plt.close()\n",
    "   \n",
    "texasKeywords = pd.read_csv(\"data/Texas-Keywords.csv\")\n",
    "texasColumns = (texasKeywords.columns)\n",
    "texasColumns = texasColumns.to_numpy()\n",
    "print(texasColumns)\n",
    "texasKeywords = texasKeywords.to_numpy() \n",
    "\n",
    "\n",
    "#print(texasKeywords.transpose())\n",
    "\n",
    "for x in range(0, len(texasKeywords.transpose())): \n",
    "    a = texasKeywords.transpose()[x]\n",
    "    a = list(a) \n",
    "   # print(a) \n",
    "    a = filter(lambda x: type(x) == str, a)\n",
    "    a = list(a)\n",
    "    \n",
    "    print(a)\n",
    "    processTexas(\"data/earthquakes_log.csv\", \"data/twitter/tweetdata.csv\", texasColumns[x], a) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
